The terminology of machine learning is hopelessly convoluted. We try to use consistent phrases and notation as much as possible, but we don't always manage. Here is a brief, alphabetic list with short explanations of terms, common synonyms and references to where they are explained.

**This page is editable for everybody**, so if you see something you don't understand, please add it, and we'll try to explain it. If you think you can contribute in another way, please, [be **bold**](https://en.wikipedia.org/wiki/Wikipedia:Be_bold).

## Phrases

- [[Linearly Independent Sets; Bases|Basis]]: A set of vectors (the basis vectors) that is used as units in a coordinate system. Every point is expressed as a linear combination of the basis vectors.
- **Bayesianism**, A subjectivist notion of probability. Leads to methods where we never look for a single choice of parameters, but always a distribution over parameters. See also posterior/prior.
- [[Bayes' Theorem]] a probabilistic law that tells you how to update your beliefs after seeing some data. For instance, your may have a *prior belief* that black and white swans are equally likely, but after seeing 100 white swans, your belief will change, the result is your *posterior belief*. You can also think of Bayes rule as a way to reverse conditional probabilities (the inversion problem): if you know the probability p(x|θ) of your data x given the model parameters θ, but you want to know the probability of the parameters given the data, bayes rule gives you:$$p \left(\right. \theta \mid x \left.\right) = \frac{p \left(\right. x \mid \theta \left.\right) p \left(\right. \theta \left.\right)}{p \left(\right. x \left.\right)}$$
- **Bias** (1) A constant term, independent of the input, added to the output of a regression model or neural network layer. Can be represented as a node whose input is fixed to 1 (a bias node). Also known as *intercept*. (2) Structural error. The part of the error that doesn't go away when multiple trials are averaged. The other aspect of the error is the *variance*.
- **Coverage Matrix** A way of visualizing the performance of a ranking classifier. Since we don't know the true ranking, we can only tell that a ranking classifier makes a mistake if it ranks a positive instance as more negative than a negative instance. The coverage matrix  is a matrix of all positive versus all negative instances in the test set. A cell is colored red if the corresponding pair is ranked the wrong way around, and green if it is ranked the right way around.
- **Data space,** instance space. The space of all possible instances *before* features have been extracted (all emails, all people, all chess positions). Usually a highly abstract notion, not used a lot.
- **Decision boundary**, decision surface. The shape that a binary classifier draws in feature space to distinguish the positive from the negative examples. The possible shapes depend on the model class. For a linear classifier in a 2 dimensional feature space, the shape is always a line.
- **Discriminative classifier**. A probabilistic classifier that learns the function p(c|x) (the probability of the class given the features) directly, without learning an intermediate distribution. Logistic regression produces a discriminative classifier. Contrast with a *generative classifier*.
- **Dot product,** a way of multiplying two vectors of equal length which results in a a single number. Written as $w \cdot x$ or as $w^{T} x$. See [this article](https://betterexplained.com/articles/vector-calculus-understanding-the-dot-product/) for an extensive explanation.
- **Feature space,** instance space. The space where your features are axes and every instance is a point. If you have two features, a scatter plot shows you feature space. Lecture 1.
- **Frequentism**, an objectivist notion of probability. Holds that the 'true' parameters of a models are a fixed value, so no probability can be expressed for them, they can only be estimated. Leads to methods where a single best parameter estimate is computed to fit a model to data. See also **Maximum  Likelihood**, **Likelihood**.
- **Generative model**. A probability model that is used to sample new instances that are (ideally) drawn from the same distribution of the data. For instance, a model that can generate human faces, after training on a set of images of human faces.
- **Generative classifier**. A probabilistic classifier that learns the function p(x|c) (the probability of the data given the class) and then reverses this using Bayes' rule to produce the probability of the class given the data. For example: a Bayes classifier, a Naive Bayes classifier. Contrast with a *discriminative classifier*.
- **Gradient descent**. Currently the most popular learning method. Searches a continuous model space by computing the gradient of the loss with respect to the model parameters and taking a small step in the opposite direction. Lecture 21.
- **Grading model.** Models that apply a smooth function over the feature space, and that can potentially give different predictions for instances that are arbitrarily close together.
- **Grouping models.** Models that segment the feature space into a finite set of "segments" and apply the same prediction to each segment.
- **Instances** aka examples or subjects. The things you will show to your machine learning algorithm. Usually so that the algorithm will learn to label them with classes or some other target values. Lecture 11.
- **Instance space** The space of all possible learning examples. This may refer to the data space or the feature space (it's usually clear from context).
- **Kernel** (1) A function which computes a scalar value for two feature vectors, and behaves like the dot products of those two vectors in an expanded feature space. Used in SVMs with the **kernel trick**. (2) The set of weights in a convolutional neural network layer which is applied independently to each patch of an image (short for convolutional kernel).
- **kernel size** In definition (2) of a kernel, the size of the patch to which the kernel is applied. 3x3 is a common value for 2D convolutions.
- **Likelihood**: the probability of the data given a model's parameters p(x|θ). In *frequentist* approaches, we often choose θ to maximize the likelihood. In *Bayesian* approaches, **the prior** and the likelihood together give us **the posterior probability**, which tells us everything we want to know about the parameters.
- **Loss function**, error function, objective function. The function that defines how good a specific model is. The loss has a model as its argument, and the data as a constant. Most learning boils down to defining a loss function, and searching the model space for a model with low loss. Lecture 12.
- **Maximum Likelihood**, a common principle used to fit a model to data in frequentist approaches. Pick the model for which the probability of the data given the model is maximal. See also **likelihood**.
- **Mode collapse.** When a generative model produces the average of the data, instead of specific high-probability samples. For instance, a model trained to produce faces should choose to produce either a male of a female person, and make that choice again for each new sample. If instead, the model produces a sort of average-gendered person, that doesn't differ between samples, this is called mode collapse. The instance space has many different regions of high probability, the *modes,* and instead of modelling these, the model just puts a single region of high probability in the middle, equidistant to each mode.
- **Model space,** hypothesis space. The abstract idea of "all models under consideration." If your model is a line, your model space is the set of all possible lines. In some cases (e.g. lines) your model space can be represented as a line (like a line, plane or 3d space). In other cases (like decision trees) it can't. In some cases (kNN) it can be a little opaque what the model space is exactly. See Lecture 11.
- **Normal distribution**, MVN, Gaussian. A very common probability distribution. On one feature, it's called a univariate normal, over several, it's called a multivariate normal distribution.
- **One-hot coding**, 1-of-N coding. Turning a categorical feature into several numeric features. See slide 19 in lecture *22 Methodology 2*.
- **PCA** Principal Component Analysis. A dimensionality reduction method. Maps the original feature vectors to new feature vectors called principal components. The first principal component is the linear reduction to one dimension that minimizes squared error of the reconstructed data with the original. Equivalently, it maximizes the variance of the reduced data. The second principal component adds the maximal remaining variance orthogonal to that, and so on.
- **Prior probability**, The probability expressing our belief over a model's parameters before we've seen the data: p(θ).
- **Posterior probability**, The probability expressing our belief over a model's parameters after we've seen the data: p(θ|x).
- **Ranking classifier**, scoring classifier. A classifier that not only assigns classes, but also assigns a degree to which each point belongs to the assigned class. This allows it, for instance, to rank the instances from least spam to most spam. Such a classifier is required to compute the AUC score. Many classifiers like linear models and decisions trees have specific methods to turn the trained classifier into a ranking classifier.
- **Ranking error.** A specific type of mistake made by a ranking classifier. Every pair of instances of different classes (one positive, one negative) is a ranking error if the negative example is ranked as more positive than the positive; i.e. if the pair is ranked the wrong way around. One instance can be part of several ranking errors.
- **Refinement.** at how small a scale a model form a particular class can tell instances apart. Not a number you can actually compute, more of an informal property.
- **Scalar.** A number. This is usually used to distinguish a variable from a vector or a matrix. That is tensors of decreasing rank are: a matrix, a vector, a scalar.
- **Standard Normal distribution**. In the *univariate* (1 dimensional) case: the normal distribution with mean 0 and standard deviation 1. In the *multivariate* case, the normal distribution with mean at the origin, and the identity matrix as covariance matrix.
- **Vanishing gradients.** When the gradient of the loss becomes zero as it is propagated down a neural network, not because it is truly zero, but due to the limited precision of floating point representations. This often happens when deep neural networks are improperly initialized, and with nonlinearites like the sigmoid and than functions which compress the input to a finite range.
- **Weights, parameters,** coefficients. The  set of values that together determine a single choice of model.

## Notation

- $w \cdot x$ The **dot product** of two vectors. Also written as $w^{T} x$
- $\propto$ Read as *proportional to*. This is used to simplify equations. For instance, in the equation $y = \frac{1}{\sqrt{2 \pi}} e^{- x}$ the green part is a constant. Instead of saying that the y is equal to the right hand side, we can say it's *proportional to* $e^{- x}$or, using $y \propto e^{- x}$. This just says that the two sides aren't equal, but the difference is a multiplier that doesn't depend on the value of x or y.
